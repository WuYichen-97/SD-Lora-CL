seed [1993]
devices_type ['1']
2025-04-12 15:31:24,294 [trainer.py] => config: ./exps/finetune_ina1.json
2025-04-12 15:31:24,331 [trainer.py] => seed: 1993
2025-04-12 15:31:24,331 [trainer.py] => prefix: reproduce
2025-04-12 15:31:24,331 [trainer.py] => dataset: imageneta
2025-04-12 15:31:24,331 [trainer.py] => memory_size: 0
2025-04-12 15:31:24,331 [trainer.py] => memory_per_class: 0
2025-04-12 15:31:24,331 [trainer.py] => fixed_memory: False
2025-04-12 15:31:24,331 [trainer.py] => shuffle: True
2025-04-12 15:31:24,331 [trainer.py] => init_cls: 20
2025-04-12 15:31:24,331 [trainer.py] => increment: 20
2025-04-12 15:31:24,331 [trainer.py] => model_name: finetune
2025-04-12 15:31:24,331 [trainer.py] => backbone_type: vit_base_patch16_224
2025-04-12 15:31:24,331 [trainer.py] => device: [device(type='cuda', index=1)]
2025-04-12 15:31:24,331 [trainer.py] => optimizer: adam
2025-04-12 15:31:24,331 [trainer.py] => scheduler: constant
2025-04-12 15:31:24,331 [trainer.py] => filepath: ./ImageNetA1/
2025-04-12 15:31:24,331 [trainer.py] => init_epoch: 20
2025-04-12 15:31:24,331 [trainer.py] => init_lr: 0.01
2025-04-12 15:31:24,331 [trainer.py] => init_milestones: [10000]
2025-04-12 15:31:24,332 [trainer.py] => init_lr_decay: 0
2025-04-12 15:31:24,332 [trainer.py] => init_weight_decay: 0.0005
2025-04-12 15:31:24,332 [trainer.py] => epochs: 20
2025-04-12 15:31:24,332 [trainer.py] => lrate: 0.01
2025-04-12 15:31:24,332 [trainer.py] => milestones: [10000]
2025-04-12 15:31:24,332 [trainer.py] => lrate_decay: 0
2025-04-12 15:31:24,332 [trainer.py] => batch_size: 128
2025-04-12 15:31:24,332 [trainer.py] => weight_decay: 0.0002
2025-04-12 15:31:24,457 [data_manager.py] => [168, 136, 51, 9, 183, 101, 171, 99, 42, 159, 191, 70, 16, 188, 27, 10, 175, 26, 68, 187, 98, 6, 85, 35, 112, 43, 100, 0, 103, 181, 88, 59, 4, 2, 116, 174, 94, 80, 106, 1, 147, 17, 141, 131, 72, 23, 173, 54, 197, 118, 87, 32, 79, 104, 91, 19, 135, 107, 178, 36, 11, 199, 142, 8, 122, 3, 28, 57, 153, 172, 190, 56, 49, 44, 97, 62, 151, 169, 194, 55, 192, 12, 189, 78, 66, 180, 15, 137, 109, 134, 92, 119, 126, 52, 170, 40, 148, 65, 144, 64, 138, 45, 77, 89, 154, 90, 71, 193, 74, 30, 113, 143, 96, 84, 67, 50, 186, 156, 69, 21, 18, 111, 108, 58, 125, 157, 150, 110, 182, 129, 166, 83, 81, 60, 13, 165, 14, 176, 63, 117, 5, 22, 145, 121, 38, 41, 82, 127, 114, 20, 31, 53, 37, 163, 196, 130, 152, 162, 86, 76, 24, 34, 184, 149, 33, 128, 198, 155, 146, 167, 139, 120, 140, 102, 47, 25, 158, 123, 46, 164, 61, 7, 115, 75, 133, 160, 105, 132, 179, 124, 48, 73, 93, 39, 95, 195, 29, 177, 185, 161]
!!!!!!! multiple_gpus [device(type='cuda', index=1)]
This is for the BaseNet initialization.
2025-04-12 15:31:33,377 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:31:34,418 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/
Initialize task-id and curtask id
After BaseNet initialization.
task 0
2025-04-12 15:31:38,611 [trainer.py] => All params: 171965973
2025-04-12 15:31:38,612 [trainer.py] => Trainable params: 368661
2025-04-12 15:31:38,612 [finetune.py] => Learning on 0-20
/n/home02/ycwu/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(

  0%|          | 0/20 [00:00<?, ?it/s]
Task 0, Epoch 1/20 => Loss 2.847, Train_accy 20.00, Test_accy 50.86:   0%|          | 0/20 [00:23<?, ?it/s]
Task 0, Epoch 1/20 => Loss 2.847, Train_accy 20.00, Test_accy 50.86:   5%|▌         | 1/20 [00:23<07:35, 23.98s/it]
Task 0, Epoch 2/20 => Loss 1.382, Train_accy 60.99:   5%|▌         | 1/20 [00:36<07:35, 23.98s/it]                 
Task 0, Epoch 2/20 => Loss 1.382, Train_accy 60.99:  10%|█         | 2/20 [00:36<05:10, 17.26s/it]
Task 0, Epoch 3/20 => Loss 0.889, Train_accy 73.19:  10%|█         | 2/20 [00:47<05:10, 17.26s/it]
Task 0, Epoch 3/20 => Loss 0.889, Train_accy 73.19:  15%|█▌        | 3/20 [00:47<04:06, 14.49s/it]
Task 0, Epoch 4/20 => Loss 0.645, Train_accy 80.99:  15%|█▌        | 3/20 [00:57<04:06, 14.49s/it]
Task 0, Epoch 4/20 => Loss 0.645, Train_accy 80.99:  20%|██        | 4/20 [00:57<03:23, 12.73s/it]
Task 0, Epoch 5/20 => Loss 0.531, Train_accy 84.82:  20%|██        | 4/20 [01:07<03:23, 12.73s/it]
Task 0, Epoch 5/20 => Loss 0.531, Train_accy 84.82:  25%|██▌       | 5/20 [01:07<02:52, 11.48s/it]
Task 0, Epoch 6/20 => Loss 0.426, Train_accy 87.80, Test_accy 77.14:  25%|██▌       | 5/20 [01:23<02:52, 11.48s/it]
Task 0, Epoch 6/20 => Loss 0.426, Train_accy 87.80, Test_accy 77.14:  30%|███       | 6/20 [01:23<03:06, 13.29s/it]
Task 0, Epoch 7/20 => Loss 0.389, Train_accy 88.94:  30%|███       | 6/20 [01:36<03:06, 13.29s/it]                 
Task 0, Epoch 7/20 => Loss 0.389, Train_accy 88.94:  35%|███▌      | 7/20 [01:36<02:50, 13.08s/it]
Task 0, Epoch 8/20 => Loss 0.302, Train_accy 90.78:  35%|███▌      | 7/20 [01:47<02:50, 13.08s/it]
Task 0, Epoch 8/20 => Loss 0.302, Train_accy 90.78:  40%|████      | 8/20 [01:47<02:28, 12.38s/it]
Task 0, Epoch 9/20 => Loss 0.329, Train_accy 89.79:  40%|████      | 8/20 [01:58<02:28, 12.38s/it]
Task 0, Epoch 9/20 => Loss 0.329, Train_accy 89.79:  45%|████▌     | 9/20 [01:58<02:10, 11.88s/it]
Task 0, Epoch 10/20 => Loss 0.321, Train_accy 89.65:  45%|████▌     | 9/20 [02:07<02:10, 11.88s/it]
Task 0, Epoch 10/20 => Loss 0.321, Train_accy 89.65:  50%|█████     | 10/20 [02:07<01:51, 11.12s/it]
Task 0, Epoch 11/20 => Loss 0.254, Train_accy 91.77, Test_accy 78.29:  50%|█████     | 10/20 [02:25<01:51, 11.12s/it]
Task 0, Epoch 11/20 => Loss 0.254, Train_accy 91.77, Test_accy 78.29:  55%|█████▌    | 11/20 [02:25<01:58, 13.12s/it]
Task 0, Epoch 12/20 => Loss 0.250, Train_accy 92.62:  55%|█████▌    | 11/20 [02:35<01:58, 13.12s/it]                 
Task 0, Epoch 12/20 => Loss 0.250, Train_accy 92.62:  60%|██████    | 12/20 [02:35<01:36, 12.10s/it]
Task 0, Epoch 13/20 => Loss 0.246, Train_accy 92.62:  60%|██████    | 12/20 [02:43<01:36, 12.10s/it]
Task 0, Epoch 13/20 => Loss 0.246, Train_accy 92.62:  65%|██████▌   | 13/20 [02:43<01:18, 11.16s/it]
Task 0, Epoch 14/20 => Loss 0.225, Train_accy 93.19:  65%|██████▌   | 13/20 [02:54<01:18, 11.16s/it]
Task 0, Epoch 14/20 => Loss 0.225, Train_accy 93.19:  70%|███████   | 14/20 [02:54<01:05, 10.85s/it]
Task 0, Epoch 15/20 => Loss 0.251, Train_accy 93.62:  70%|███████   | 14/20 [03:04<01:05, 10.85s/it]
Task 0, Epoch 15/20 => Loss 0.251, Train_accy 93.62:  75%|███████▌  | 15/20 [03:04<00:52, 10.58s/it]
Task 0, Epoch 16/20 => Loss 0.256, Train_accy 93.05, Test_accy 80.57:  75%|███████▌  | 15/20 [03:23<00:52, 10.58s/it]
Task 0, Epoch 16/20 => Loss 0.256, Train_accy 93.05, Test_accy 80.57:  80%|████████  | 16/20 [03:23<00:52, 13.16s/it]
Task 0, Epoch 17/20 => Loss 0.224, Train_accy 93.33:  80%|████████  | 16/20 [03:33<00:52, 13.16s/it]                 
Task 0, Epoch 17/20 => Loss 0.224, Train_accy 93.33:  85%|████████▌ | 17/20 [03:33<00:36, 12.23s/it]
Task 0, Epoch 18/20 => Loss 0.270, Train_accy 91.63:  85%|████████▌ | 17/20 [03:41<00:36, 12.23s/it]
Task 0, Epoch 18/20 => Loss 0.270, Train_accy 91.63:  90%|█████████ | 18/20 [03:41<00:22, 11.01s/it]
Task 0, Epoch 19/20 => Loss 0.204, Train_accy 93.19:  90%|█████████ | 18/20 [03:51<00:22, 11.01s/it]
Task 0, Epoch 19/20 => Loss 0.204, Train_accy 93.19:  95%|█████████▌| 19/20 [03:51<00:10, 10.86s/it]
Task 0, Epoch 20/20 => Loss 0.229, Train_accy 92.91:  95%|█████████▌| 19/20 [04:01<00:10, 10.86s/it]
Task 0, Epoch 20/20 => Loss 0.229, Train_accy 92.91: 100%|██████████| 20/20 [04:01<00:00, 10.47s/it]
Task 0, Epoch 20/20 => Loss 0.229, Train_accy 92.91: 100%|██████████| 20/20 [04:01<00:00, 12.08s/it]
2025-04-12 15:35:42,826 [finetune.py] => Task 0, Epoch 20/20 => Loss 0.229, Train_accy 92.91
2025-04-12 15:35:47,972 [trainer.py] => No NME accuracy.
2025-04-12 15:35:47,972 [trainer.py] => CNN: {'total': np.float64(82.29), '00-19': np.float64(82.29), 'old': 0, 'new': np.float64(82.29)}
2025-04-12 15:35:47,972 [trainer.py] => CNN top1 curve: [np.float64(82.29)]
2025-04-12 15:35:47,972 [trainer.py] => CNN top5 curve: [np.float64(97.71)]

Average Accuracy (CNN): 82.29
2025-04-12 15:35:47,972 [trainer.py] => Average Accuracy (CNN): 82.29 

task 1
2025-04-12 15:35:47,973 [trainer.py] => All params: 171981353
2025-04-12 15:35:47,974 [trainer.py] => Trainable params: 384041
2025-04-12 15:35:47,975 [finetune.py] => Learning on 20-40
2025-04-12 15:35:49,124 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:35:49,195 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/yichenwu/LoRA-CL1/backbone/lora.py:377: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_lora_A['saved_A_'+str(i)] = torch.load(file_path)
/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/yichenwu/LoRA-CL1/backbone/lora.py:379: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_lora_B['saved_B_'+str(i)] = torch.load(file_path)
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/yichenwu/LoRA-CL1/backbone/lora.py:544: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  w_As = torch.load(file_path)

Task 1, Epoch 1/20 => Loss 3.092, Train_accy 2.87, Test_accy 48.33:   0%|          | 0/20 [00:21<?, ?it/s]
Task 1, Epoch 1/20 => Loss 3.092, Train_accy 2.87, Test_accy 48.33:   5%|▌         | 1/20 [00:21<06:45, 21.36s/it]
Task 1, Epoch 2/20 => Loss 1.534, Train_accy 30.29:   5%|▌         | 1/20 [00:32<06:45, 21.36s/it]                
Task 1, Epoch 2/20 => Loss 1.534, Train_accy 30.29:  10%|█         | 2/20 [00:32<04:40, 15.61s/it]
Task 1, Epoch 3/20 => Loss 0.947, Train_accy 54.31:  10%|█         | 2/20 [00:43<04:40, 15.61s/it]
Task 1, Epoch 3/20 => Loss 0.947, Train_accy 54.31:  15%|█▌        | 3/20 [00:43<03:44, 13.23s/it]
Task 1, Epoch 4/20 => Loss 0.750, Train_accy 64.36:  15%|█▌        | 3/20 [00:53<03:44, 13.23s/it]
Task 1, Epoch 4/20 => Loss 0.750, Train_accy 64.36:  20%|██        | 4/20 [00:53<03:15, 12.19s/it]
Task 1, Epoch 5/20 => Loss 0.527, Train_accy 73.50:  20%|██        | 4/20 [01:05<03:15, 12.19s/it]
Task 1, Epoch 5/20 => Loss 0.527, Train_accy 73.50:  25%|██▌       | 5/20 [01:05<03:00, 12.05s/it]
Task 1, Epoch 6/20 => Loss 0.487, Train_accy 77.42, Test_accy 77.22:  25%|██▌       | 5/20 [01:25<03:00, 12.05s/it]
Task 1, Epoch 6/20 => Loss 0.487, Train_accy 77.42, Test_accy 77.22:  30%|███       | 6/20 [01:25<03:27, 14.79s/it]
Task 1, Epoch 7/20 => Loss 0.464, Train_accy 77.42:  30%|███       | 6/20 [01:37<03:27, 14.79s/it]                 
Task 1, Epoch 7/20 => Loss 0.464, Train_accy 77.42:  35%|███▌      | 7/20 [01:37<02:58, 13.73s/it]
Task 1, Epoch 8/20 => Loss 0.426, Train_accy 80.16:  35%|███▌      | 7/20 [01:47<02:58, 13.73s/it]
Task 1, Epoch 8/20 => Loss 0.426, Train_accy 80.16:  40%|████      | 8/20 [01:47<02:32, 12.71s/it]
Task 1, Epoch 9/20 => Loss 0.395, Train_accy 81.20:  40%|████      | 8/20 [01:58<02:32, 12.71s/it]
Task 1, Epoch 9/20 => Loss 0.395, Train_accy 81.20:  45%|████▌     | 9/20 [01:58<02:13, 12.09s/it]
Task 1, Epoch 10/20 => Loss 0.386, Train_accy 79.50:  45%|████▌     | 9/20 [02:10<02:13, 12.09s/it]
Task 1, Epoch 10/20 => Loss 0.386, Train_accy 79.50:  50%|█████     | 10/20 [02:10<01:59, 11.99s/it]
Task 1, Epoch 11/20 => Loss 0.337, Train_accy 80.55, Test_accy 79.17:  50%|█████     | 10/20 [02:31<01:59, 11.99s/it]
Task 1, Epoch 11/20 => Loss 0.337, Train_accy 80.55, Test_accy 79.17:  55%|█████▌    | 11/20 [02:31<02:11, 14.64s/it]
Task 1, Epoch 12/20 => Loss 0.325, Train_accy 80.94:  55%|█████▌    | 11/20 [02:41<02:11, 14.64s/it]                 
Task 1, Epoch 12/20 => Loss 0.325, Train_accy 80.94:  60%|██████    | 12/20 [02:41<01:45, 13.24s/it]
Task 1, Epoch 13/20 => Loss 0.370, Train_accy 80.55:  60%|██████    | 12/20 [02:52<01:45, 13.24s/it]
Task 1, Epoch 13/20 => Loss 0.370, Train_accy 80.55:  65%|██████▌   | 13/20 [02:52<01:29, 12.83s/it]
Task 1, Epoch 14/20 => Loss 0.327, Train_accy 82.90:  65%|██████▌   | 13/20 [03:05<01:29, 12.83s/it]
Task 1, Epoch 14/20 => Loss 0.327, Train_accy 82.90:  70%|███████   | 14/20 [03:05<01:16, 12.81s/it]
Task 1, Epoch 15/20 => Loss 0.292, Train_accy 85.25:  70%|███████   | 14/20 [03:16<01:16, 12.81s/it]
Task 1, Epoch 15/20 => Loss 0.292, Train_accy 85.25:  75%|███████▌  | 15/20 [03:16<01:00, 12.13s/it]
Task 1, Epoch 16/20 => Loss 0.251, Train_accy 84.99, Test_accy 78.89:  75%|███████▌  | 15/20 [03:36<01:00, 12.13s/it]
Task 1, Epoch 16/20 => Loss 0.251, Train_accy 84.99, Test_accy 78.89:  80%|████████  | 16/20 [03:36<00:58, 14.50s/it]
Task 1, Epoch 17/20 => Loss 0.294, Train_accy 82.38:  80%|████████  | 16/20 [03:49<00:58, 14.50s/it]                 
Task 1, Epoch 17/20 => Loss 0.294, Train_accy 82.38:  85%|████████▌ | 17/20 [03:49<00:42, 14.08s/it]
Task 1, Epoch 18/20 => Loss 0.287, Train_accy 84.33:  85%|████████▌ | 17/20 [04:01<00:42, 14.08s/it]
Task 1, Epoch 18/20 => Loss 0.287, Train_accy 84.33:  90%|█████████ | 18/20 [04:01<00:26, 13.44s/it]
Task 1, Epoch 19/20 => Loss 0.222, Train_accy 84.86:  90%|█████████ | 18/20 [04:12<00:26, 13.44s/it]
Task 1, Epoch 19/20 => Loss 0.222, Train_accy 84.86:  95%|█████████▌| 19/20 [04:12<00:12, 12.64s/it]
Task 1, Epoch 20/20 => Loss 0.295, Train_accy 84.73:  95%|█████████▌| 19/20 [04:22<00:12, 12.64s/it]
Task 1, Epoch 20/20 => Loss 0.295, Train_accy 84.73: 100%|██████████| 20/20 [04:22<00:00, 12.04s/it]
Task 1, Epoch 20/20 => Loss 0.295, Train_accy 84.73: 100%|██████████| 20/20 [04:22<00:00, 13.14s/it]
2025-04-12 15:40:12,296 [finetune.py] => Task 1, Epoch 20/20 => Loss 0.295, Train_accy 84.73
2025-04-12 15:40:20,804 [trainer.py] => No NME accuracy.
2025-04-12 15:40:20,804 [trainer.py] => CNN: {'total': np.float64(78.33), '00-19': np.float64(78.86), '20-39': np.float64(77.84), 'old': np.float64(78.86), 'new': np.float64(77.84)}
2025-04-12 15:40:20,804 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33)]
2025-04-12 15:40:20,804 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72)]

Average Accuracy (CNN): 80.31
2025-04-12 15:40:20,804 [trainer.py] => Average Accuracy (CNN): 80.31 

task 2
2025-04-12 15:40:20,806 [trainer.py] => All params: 171996733
2025-04-12 15:40:20,807 [trainer.py] => Trainable params: 399421
2025-04-12 15:40:20,808 [finetune.py] => Learning on 40-60
2025-04-12 15:40:23,233 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:40:23,343 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 2, Epoch 1/20 => Loss 3.245, Train_accy 0.20, Test_accy 60.08:   0%|          | 0/20 [00:20<?, ?it/s]
Task 2, Epoch 1/20 => Loss 3.245, Train_accy 0.20, Test_accy 60.08:   5%|▌         | 1/20 [00:20<06:22, 20.13s/it]
Task 2, Epoch 2/20 => Loss 1.637, Train_accy 15.07:   5%|▌         | 1/20 [00:30<06:22, 20.13s/it]                
Task 2, Epoch 2/20 => Loss 1.637, Train_accy 15.07:  10%|█         | 2/20 [00:30<04:22, 14.58s/it]
Task 2, Epoch 3/20 => Loss 0.995, Train_accy 40.70:  10%|█         | 2/20 [00:40<04:22, 14.58s/it]
Task 2, Epoch 3/20 => Loss 0.995, Train_accy 40.70:  15%|█▌        | 3/20 [00:40<03:30, 12.40s/it]
Task 2, Epoch 4/20 => Loss 0.738, Train_accy 60.47:  15%|█▌        | 3/20 [00:51<03:30, 12.40s/it]
Task 2, Epoch 4/20 => Loss 0.738, Train_accy 60.47:  20%|██        | 4/20 [00:51<03:09, 11.83s/it]
Task 2, Epoch 5/20 => Loss 0.605, Train_accy 65.56:  20%|██        | 4/20 [01:01<03:09, 11.83s/it]
Task 2, Epoch 5/20 => Loss 0.605, Train_accy 65.56:  25%|██▌       | 5/20 [01:01<02:45, 11.06s/it]
Task 2, Epoch 6/20 => Loss 0.439, Train_accy 72.80, Test_accy 72.90:  25%|██▌       | 5/20 [01:20<02:45, 11.06s/it]
Task 2, Epoch 6/20 => Loss 0.439, Train_accy 72.80, Test_accy 72.90:  30%|███       | 6/20 [01:20<03:11, 13.69s/it]
Task 2, Epoch 7/20 => Loss 0.376, Train_accy 77.10:  30%|███       | 6/20 [01:28<03:11, 13.69s/it]                 
Task 2, Epoch 7/20 => Loss 0.376, Train_accy 77.10:  35%|███▌      | 7/20 [01:28<02:35, 12.00s/it]
Task 2, Epoch 8/20 => Loss 0.295, Train_accy 79.26:  35%|███▌      | 7/20 [01:37<02:35, 12.00s/it]
Task 2, Epoch 8/20 => Loss 0.295, Train_accy 79.26:  40%|████      | 8/20 [01:37<02:13, 11.11s/it]
Task 2, Epoch 9/20 => Loss 0.328, Train_accy 82.19:  40%|████      | 8/20 [01:46<02:13, 11.11s/it]
Task 2, Epoch 9/20 => Loss 0.328, Train_accy 82.19:  45%|████▌     | 9/20 [01:46<01:55, 10.48s/it]
Task 2, Epoch 10/20 => Loss 0.245, Train_accy 83.76:  45%|████▌     | 9/20 [01:55<01:55, 10.48s/it]
Task 2, Epoch 10/20 => Loss 0.245, Train_accy 83.76:  50%|█████     | 10/20 [01:55<01:38,  9.89s/it]
Task 2, Epoch 11/20 => Loss 0.251, Train_accy 81.41, Test_accy 74.16:  50%|█████     | 10/20 [02:11<01:38,  9.89s/it]
Task 2, Epoch 11/20 => Loss 0.251, Train_accy 81.41, Test_accy 74.16:  55%|█████▌    | 11/20 [02:11<01:47, 11.89s/it]
Task 2, Epoch 12/20 => Loss 0.258, Train_accy 84.15:  55%|█████▌    | 11/20 [02:21<01:47, 11.89s/it]                 
Task 2, Epoch 12/20 => Loss 0.258, Train_accy 84.15:  60%|██████    | 12/20 [02:21<01:28, 11.11s/it]
Task 2, Epoch 13/20 => Loss 0.214, Train_accy 86.69:  60%|██████    | 12/20 [02:29<01:28, 11.11s/it]
Task 2, Epoch 13/20 => Loss 0.214, Train_accy 86.69:  65%|██████▌   | 13/20 [02:29<01:12, 10.29s/it]
Task 2, Epoch 14/20 => Loss 0.187, Train_accy 84.93:  65%|██████▌   | 13/20 [02:38<01:12, 10.29s/it]
Task 2, Epoch 14/20 => Loss 0.187, Train_accy 84.93:  70%|███████   | 14/20 [02:38<00:59,  9.98s/it]
Task 2, Epoch 15/20 => Loss 0.166, Train_accy 86.89:  70%|███████   | 14/20 [02:48<00:59,  9.98s/it]
Task 2, Epoch 15/20 => Loss 0.166, Train_accy 86.89:  75%|███████▌  | 15/20 [02:48<00:49,  9.94s/it]
Task 2, Epoch 16/20 => Loss 0.188, Train_accy 87.67, Test_accy 75.21:  75%|███████▌  | 15/20 [03:04<00:49,  9.94s/it]
Task 2, Epoch 16/20 => Loss 0.188, Train_accy 87.67, Test_accy 75.21:  80%|████████  | 16/20 [03:04<00:46, 11.60s/it]
Task 2, Epoch 17/20 => Loss 0.203, Train_accy 88.26:  80%|████████  | 16/20 [03:13<00:46, 11.60s/it]                 
Task 2, Epoch 17/20 => Loss 0.203, Train_accy 88.26:  85%|████████▌ | 17/20 [03:13<00:33, 11.05s/it]
Task 2, Epoch 18/20 => Loss 0.146, Train_accy 87.87:  85%|████████▌ | 17/20 [03:22<00:33, 11.05s/it]
Task 2, Epoch 18/20 => Loss 0.146, Train_accy 87.87:  90%|█████████ | 18/20 [03:22<00:20, 10.43s/it]
Task 2, Epoch 19/20 => Loss 0.179, Train_accy 86.89:  90%|█████████ | 18/20 [03:32<00:20, 10.43s/it]
Task 2, Epoch 19/20 => Loss 0.179, Train_accy 86.89:  95%|█████████▌| 19/20 [03:32<00:10, 10.20s/it]
Task 2, Epoch 20/20 => Loss 0.142, Train_accy 88.45:  95%|█████████▌| 19/20 [03:42<00:10, 10.20s/it]
Task 2, Epoch 20/20 => Loss 0.142, Train_accy 88.45: 100%|██████████| 20/20 [03:42<00:00, 10.16s/it]
Task 2, Epoch 20/20 => Loss 0.142, Train_accy 88.45: 100%|██████████| 20/20 [03:42<00:00, 11.13s/it]
2025-04-12 15:44:06,720 [finetune.py] => Task 2, Epoch 20/20 => Loss 0.142, Train_accy 88.45
2025-04-12 15:44:12,955 [trainer.py] => No NME accuracy.
2025-04-12 15:44:12,956 [trainer.py] => CNN: {'total': np.float64(74.58), '00-19': np.float64(75.43), '20-39': np.float64(72.97), '40-59': np.float64(75.86), 'old': np.float64(74.17), 'new': np.float64(75.86)}
2025-04-12 15:44:12,956 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58)]
2025-04-12 15:44:12,956 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6)]

Average Accuracy (CNN): 78.39999999999999
2025-04-12 15:44:12,956 [trainer.py] => Average Accuracy (CNN): 78.39999999999999 

task 3
2025-04-12 15:44:12,957 [trainer.py] => All params: 172012113
2025-04-12 15:44:12,958 [trainer.py] => Trainable params: 414801
2025-04-12 15:44:12,959 [finetune.py] => Learning on 60-80
2025-04-12 15:44:16,696 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:44:16,749 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 3, Epoch 1/20 => Loss 2.573, Train_accy 1.02, Test_accy 53.72:   0%|          | 0/20 [00:22<?, ?it/s]
Task 3, Epoch 1/20 => Loss 2.573, Train_accy 1.02, Test_accy 53.72:   5%|▌         | 1/20 [00:22<06:59, 22.07s/it]
Task 3, Epoch 2/20 => Loss 1.040, Train_accy 24.62:   5%|▌         | 1/20 [00:35<06:59, 22.07s/it]                
Task 3, Epoch 2/20 => Loss 1.040, Train_accy 24.62:  10%|█         | 2/20 [00:35<05:03, 16.88s/it]
Task 3, Epoch 3/20 => Loss 0.706, Train_accy 48.34:  10%|█         | 2/20 [00:47<05:03, 16.88s/it]
Task 3, Epoch 3/20 => Loss 0.706, Train_accy 48.34:  15%|█▌        | 3/20 [00:47<04:11, 14.79s/it]
Task 3, Epoch 4/20 => Loss 0.559, Train_accy 58.67:  15%|█▌        | 3/20 [00:59<04:11, 14.79s/it]
Task 3, Epoch 4/20 => Loss 0.559, Train_accy 58.67:  20%|██        | 4/20 [00:59<03:39, 13.69s/it]
Task 3, Epoch 5/20 => Loss 0.499, Train_accy 62.50:  20%|██        | 4/20 [01:11<03:39, 13.69s/it]
Task 3, Epoch 5/20 => Loss 0.499, Train_accy 62.50:  25%|██▌       | 5/20 [01:11<03:15, 13.03s/it]
Task 3, Epoch 6/20 => Loss 0.433, Train_accy 66.58, Test_accy 71.63:  25%|██▌       | 5/20 [01:35<03:15, 13.03s/it]
Task 3, Epoch 6/20 => Loss 0.433, Train_accy 66.58, Test_accy 71.63:  30%|███       | 6/20 [01:35<03:55, 16.83s/it]
Task 3, Epoch 7/20 => Loss 0.431, Train_accy 70.15:  30%|███       | 6/20 [01:48<03:55, 16.83s/it]                 
Task 3, Epoch 7/20 => Loss 0.431, Train_accy 70.15:  35%|███▌      | 7/20 [01:48<03:21, 15.50s/it]
Task 3, Epoch 8/20 => Loss 0.310, Train_accy 67.22:  35%|███▌      | 7/20 [02:00<03:21, 15.50s/it]
Task 3, Epoch 8/20 => Loss 0.310, Train_accy 67.22:  40%|████      | 8/20 [02:00<02:54, 14.53s/it]
Task 3, Epoch 9/20 => Loss 0.322, Train_accy 71.17:  40%|████      | 8/20 [02:13<02:54, 14.53s/it]
Task 3, Epoch 9/20 => Loss 0.322, Train_accy 71.17:  45%|████▌     | 9/20 [02:13<02:33, 13.95s/it]
Task 3, Epoch 10/20 => Loss 0.327, Train_accy 70.03:  45%|████▌     | 9/20 [02:26<02:33, 13.95s/it]
Task 3, Epoch 10/20 => Loss 0.327, Train_accy 70.03:  50%|█████     | 10/20 [02:26<02:14, 13.49s/it]
Task 3, Epoch 11/20 => Loss 0.326, Train_accy 69.26, Test_accy 71.49:  50%|█████     | 10/20 [02:49<02:14, 13.49s/it]
Task 3, Epoch 11/20 => Loss 0.326, Train_accy 69.26, Test_accy 71.49:  55%|█████▌    | 11/20 [02:49<02:28, 16.54s/it]
Task 3, Epoch 12/20 => Loss 0.260, Train_accy 71.05:  55%|█████▌    | 11/20 [03:01<02:28, 16.54s/it]                 
Task 3, Epoch 12/20 => Loss 0.260, Train_accy 71.05:  60%|██████    | 12/20 [03:01<02:00, 15.10s/it]
Task 3, Epoch 13/20 => Loss 0.358, Train_accy 71.17:  60%|██████    | 12/20 [03:13<02:00, 15.10s/it]
Task 3, Epoch 13/20 => Loss 0.358, Train_accy 71.17:  65%|██████▌   | 13/20 [03:13<01:39, 14.26s/it]
Task 3, Epoch 14/20 => Loss 0.267, Train_accy 72.70:  65%|██████▌   | 13/20 [03:26<01:39, 14.26s/it]
Task 3, Epoch 14/20 => Loss 0.267, Train_accy 72.70:  70%|███████   | 14/20 [03:26<01:23, 13.86s/it]
Task 3, Epoch 15/20 => Loss 0.261, Train_accy 72.07:  70%|███████   | 14/20 [03:38<01:23, 13.86s/it]
Task 3, Epoch 15/20 => Loss 0.261, Train_accy 72.07:  75%|███████▌  | 15/20 [03:38<01:06, 13.35s/it]
Task 3, Epoch 16/20 => Loss 0.214, Train_accy 74.11, Test_accy 71.35:  75%|███████▌  | 15/20 [04:02<01:06, 13.35s/it]
Task 3, Epoch 16/20 => Loss 0.214, Train_accy 74.11, Test_accy 71.35:  80%|████████  | 16/20 [04:02<01:06, 16.57s/it]
Task 3, Epoch 17/20 => Loss 0.230, Train_accy 72.83:  80%|████████  | 16/20 [04:14<01:06, 16.57s/it]                 
Task 3, Epoch 17/20 => Loss 0.230, Train_accy 72.83:  85%|████████▌ | 17/20 [04:14<00:45, 15.09s/it]
Task 3, Epoch 18/20 => Loss 0.229, Train_accy 71.81:  85%|████████▌ | 17/20 [04:25<00:45, 15.09s/it]
Task 3, Epoch 18/20 => Loss 0.229, Train_accy 71.81:  90%|█████████ | 18/20 [04:25<00:27, 13.89s/it]
Task 3, Epoch 19/20 => Loss 0.222, Train_accy 74.23:  90%|█████████ | 18/20 [04:36<00:27, 13.89s/it]
Task 3, Epoch 19/20 => Loss 0.222, Train_accy 74.23:  95%|█████████▌| 19/20 [04:36<00:13, 13.06s/it]
Task 3, Epoch 20/20 => Loss 0.228, Train_accy 75.13:  95%|█████████▌| 19/20 [04:48<00:13, 13.06s/it]
Task 3, Epoch 20/20 => Loss 0.228, Train_accy 75.13: 100%|██████████| 20/20 [04:48<00:00, 12.70s/it]
Task 3, Epoch 20/20 => Loss 0.228, Train_accy 75.13: 100%|██████████| 20/20 [04:48<00:00, 14.43s/it]
2025-04-12 15:49:05,696 [finetune.py] => Task 3, Epoch 20/20 => Loss 0.228, Train_accy 75.13
2025-04-12 15:49:13,936 [trainer.py] => No NME accuracy.
2025-04-12 15:49:13,936 [trainer.py] => CNN: {'total': np.float64(71.92), '00-19': np.float64(70.86), '20-39': np.float64(71.89), '40-59': np.float64(75.0), '60-79': np.float64(71.17), 'old': np.float64(72.27), 'new': np.float64(71.17)}
2025-04-12 15:49:13,936 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92)]
2025-04-12 15:49:13,936 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26)]

Average Accuracy (CNN): 76.78
2025-04-12 15:49:13,937 [trainer.py] => Average Accuracy (CNN): 76.78 

task 4
2025-04-12 15:49:13,938 [trainer.py] => All params: 172027493
2025-04-12 15:49:13,939 [trainer.py] => Trainable params: 430181
2025-04-12 15:49:13,940 [finetune.py] => Learning on 80-100
2025-04-12 15:49:18,476 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:49:18,623 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 4, Epoch 1/20 => Loss 3.019, Train_accy 0.00, Test_accy 60.64:   0%|          | 0/20 [00:20<?, ?it/s]
Task 4, Epoch 1/20 => Loss 3.019, Train_accy 0.00, Test_accy 60.64:   5%|▌         | 1/20 [00:20<06:29, 20.49s/it]
Task 4, Epoch 2/20 => Loss 1.572, Train_accy 9.16:   5%|▌         | 1/20 [00:31<06:29, 20.49s/it]                 
Task 4, Epoch 2/20 => Loss 1.572, Train_accy 9.16:  10%|█         | 2/20 [00:31<04:25, 14.77s/it]
Task 4, Epoch 3/20 => Loss 0.777, Train_accy 41.03:  10%|█         | 2/20 [00:41<04:25, 14.77s/it]
Task 4, Epoch 3/20 => Loss 0.777, Train_accy 41.03:  15%|█▌        | 3/20 [00:41<03:39, 12.91s/it]
Task 4, Epoch 4/20 => Loss 0.614, Train_accy 60.88:  15%|█▌        | 3/20 [00:52<03:39, 12.91s/it]
Task 4, Epoch 4/20 => Loss 0.614, Train_accy 60.88:  20%|██        | 4/20 [00:52<03:08, 11.81s/it]
Task 4, Epoch 5/20 => Loss 0.432, Train_accy 67.75:  20%|██        | 4/20 [01:01<03:08, 11.81s/it]
Task 4, Epoch 5/20 => Loss 0.432, Train_accy 67.75:  25%|██▌       | 5/20 [01:01<02:43, 10.93s/it]
Task 4, Epoch 6/20 => Loss 0.470, Train_accy 70.04, Test_accy 68.25:  25%|██▌       | 5/20 [01:19<02:43, 10.93s/it]
Task 4, Epoch 6/20 => Loss 0.470, Train_accy 70.04, Test_accy 68.25:  30%|███       | 6/20 [01:19<03:08, 13.44s/it]
Task 4, Epoch 7/20 => Loss 0.461, Train_accy 72.52:  30%|███       | 6/20 [01:29<03:08, 13.44s/it]                 
Task 4, Epoch 7/20 => Loss 0.461, Train_accy 72.52:  35%|███▌      | 7/20 [01:29<02:38, 12.20s/it]
Task 4, Epoch 8/20 => Loss 0.371, Train_accy 73.47:  35%|███▌      | 7/20 [01:40<02:38, 12.20s/it]
Task 4, Epoch 8/20 => Loss 0.371, Train_accy 73.47:  40%|████      | 8/20 [01:40<02:22, 11.87s/it]
Task 4, Epoch 9/20 => Loss 0.324, Train_accy 76.53:  40%|████      | 8/20 [01:50<02:22, 11.87s/it]
Task 4, Epoch 9/20 => Loss 0.324, Train_accy 76.53:  45%|████▌     | 9/20 [01:50<02:04, 11.30s/it]
Task 4, Epoch 10/20 => Loss 0.244, Train_accy 80.15:  45%|████▌     | 9/20 [02:00<02:04, 11.30s/it]
Task 4, Epoch 10/20 => Loss 0.244, Train_accy 80.15:  50%|█████     | 10/20 [02:00<01:47, 10.75s/it]
Task 4, Epoch 11/20 => Loss 0.314, Train_accy 78.82, Test_accy 68.73:  50%|█████     | 10/20 [02:17<01:47, 10.75s/it]
Task 4, Epoch 11/20 => Loss 0.314, Train_accy 78.82, Test_accy 68.73:  55%|█████▌    | 11/20 [02:17<01:55, 12.83s/it]
Task 4, Epoch 12/20 => Loss 0.372, Train_accy 79.20:  55%|█████▌    | 11/20 [02:28<01:55, 12.83s/it]                 
Task 4, Epoch 12/20 => Loss 0.372, Train_accy 79.20:  60%|██████    | 12/20 [02:28<01:38, 12.29s/it]
Task 4, Epoch 13/20 => Loss 0.228, Train_accy 82.25:  60%|██████    | 12/20 [02:39<01:38, 12.29s/it]
Task 4, Epoch 13/20 => Loss 0.228, Train_accy 82.25:  65%|██████▌   | 13/20 [02:39<01:22, 11.76s/it]
Task 4, Epoch 14/20 => Loss 0.188, Train_accy 82.06:  65%|██████▌   | 13/20 [02:50<01:22, 11.76s/it]
Task 4, Epoch 14/20 => Loss 0.188, Train_accy 82.06:  70%|███████   | 14/20 [02:50<01:09, 11.65s/it]
Task 4, Epoch 15/20 => Loss 0.194, Train_accy 77.67:  70%|███████   | 14/20 [02:59<01:09, 11.65s/it]
Task 4, Epoch 15/20 => Loss 0.194, Train_accy 77.67:  75%|███████▌  | 15/20 [02:59<00:53, 10.74s/it]
Task 4, Epoch 16/20 => Loss 0.181, Train_accy 80.34, Test_accy 68.73:  75%|███████▌  | 15/20 [03:16<00:53, 10.74s/it]
Task 4, Epoch 16/20 => Loss 0.181, Train_accy 80.34, Test_accy 68.73:  80%|████████  | 16/20 [03:16<00:50, 12.69s/it]
Task 4, Epoch 17/20 => Loss 0.172, Train_accy 82.06:  80%|████████  | 16/20 [03:26<00:50, 12.69s/it]                 
Task 4, Epoch 17/20 => Loss 0.172, Train_accy 82.06:  85%|████████▌ | 17/20 [03:26<00:35, 11.91s/it]
Task 4, Epoch 18/20 => Loss 0.268, Train_accy 82.44:  85%|████████▌ | 17/20 [03:35<00:35, 11.91s/it]
Task 4, Epoch 18/20 => Loss 0.268, Train_accy 82.44:  90%|█████████ | 18/20 [03:35<00:21, 10.98s/it]
Task 4, Epoch 19/20 => Loss 0.136, Train_accy 82.82:  90%|█████████ | 18/20 [03:43<00:21, 10.98s/it]
Task 4, Epoch 19/20 => Loss 0.136, Train_accy 82.82:  95%|█████████▌| 19/20 [03:43<00:10, 10.13s/it]
Task 4, Epoch 20/20 => Loss 0.225, Train_accy 82.63:  95%|█████████▌| 19/20 [03:53<00:10, 10.13s/it]
Task 4, Epoch 20/20 => Loss 0.225, Train_accy 82.63: 100%|██████████| 20/20 [03:53<00:00, 10.07s/it]
Task 4, Epoch 20/20 => Loss 0.225, Train_accy 82.63: 100%|██████████| 20/20 [03:53<00:00, 11.68s/it]
2025-04-12 15:53:12,482 [finetune.py] => Task 4, Epoch 20/20 => Loss 0.225, Train_accy 82.63
2025-04-12 15:53:25,049 [trainer.py] => No NME accuracy.
2025-04-12 15:53:25,049 [trainer.py] => CNN: {'total': np.float64(68.97), '00-19': np.float64(65.71), '20-39': np.float64(70.27), '40-59': np.float64(69.83), '60-79': np.float64(70.27), '80-99': np.float64(68.53), 'old': np.float64(69.05), 'new': np.float64(68.53)}
2025-04-12 15:53:25,049 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92), np.float64(68.97)]
2025-04-12 15:53:25,049 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26), np.float64(88.7)]

Average Accuracy (CNN): 75.218
2025-04-12 15:53:25,049 [trainer.py] => Average Accuracy (CNN): 75.218 

task 5
2025-04-12 15:53:25,051 [trainer.py] => All params: 172042873
2025-04-12 15:53:25,052 [trainer.py] => Trainable params: 445561
2025-04-12 15:53:25,053 [finetune.py] => Learning on 100-120
2025-04-12 15:53:26,148 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:53:26,263 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 5, Epoch 1/20 => Loss 3.000, Train_accy 0.00, Test_accy 59.79:   0%|          | 0/20 [00:23<?, ?it/s]
Task 5, Epoch 1/20 => Loss 3.000, Train_accy 0.00, Test_accy 59.79:   5%|▌         | 1/20 [00:23<07:28, 23.60s/it]
Task 5, Epoch 2/20 => Loss 1.067, Train_accy 21.22:   5%|▌         | 1/20 [00:35<07:28, 23.60s/it]                
Task 5, Epoch 2/20 => Loss 1.067, Train_accy 21.22:  10%|█         | 2/20 [00:35<05:00, 16.71s/it]
Task 5, Epoch 3/20 => Loss 0.702, Train_accy 46.27:  10%|█         | 2/20 [00:46<05:00, 16.71s/it]
Task 5, Epoch 3/20 => Loss 0.702, Train_accy 46.27:  15%|█▌        | 3/20 [00:46<03:58, 14.03s/it]
Task 5, Epoch 4/20 => Loss 0.554, Train_accy 58.51:  15%|█▌        | 3/20 [00:56<03:58, 14.03s/it]
Task 5, Epoch 4/20 => Loss 0.554, Train_accy 58.51:  20%|██        | 4/20 [00:56<03:22, 12.63s/it]
Task 5, Epoch 5/20 => Loss 0.430, Train_accy 70.55:  20%|██        | 4/20 [01:06<03:22, 12.63s/it]
Task 5, Epoch 5/20 => Loss 0.430, Train_accy 70.55:  25%|██▌       | 5/20 [01:06<02:56, 11.75s/it]
Task 5, Epoch 6/20 => Loss 0.304, Train_accy 71.51, Test_accy 65.13:  25%|██▌       | 5/20 [01:27<02:56, 11.75s/it]
Task 5, Epoch 6/20 => Loss 0.304, Train_accy 71.51, Test_accy 65.13:  30%|███       | 6/20 [01:27<03:27, 14.85s/it]
Task 5, Epoch 7/20 => Loss 0.274, Train_accy 75.14:  30%|███       | 6/20 [01:38<03:27, 14.85s/it]                 
Task 5, Epoch 7/20 => Loss 0.274, Train_accy 75.14:  35%|███▌      | 7/20 [01:38<02:53, 13.38s/it]
Task 5, Epoch 8/20 => Loss 0.364, Train_accy 73.42:  35%|███▌      | 7/20 [01:48<02:53, 13.38s/it]
Task 5, Epoch 8/20 => Loss 0.364, Train_accy 73.42:  40%|████      | 8/20 [01:48<02:29, 12.42s/it]
Task 5, Epoch 9/20 => Loss 0.234, Train_accy 78.01:  40%|████      | 8/20 [01:58<02:29, 12.42s/it]
Task 5, Epoch 9/20 => Loss 0.234, Train_accy 78.01:  45%|████▌     | 9/20 [01:58<02:07, 11.58s/it]
Task 5, Epoch 10/20 => Loss 0.286, Train_accy 77.82:  45%|████▌     | 9/20 [02:06<02:07, 11.58s/it]
Task 5, Epoch 10/20 => Loss 0.286, Train_accy 77.82:  50%|█████     | 10/20 [02:06<01:45, 10.55s/it]
Task 5, Epoch 11/20 => Loss 0.285, Train_accy 76.10, Test_accy 64.92:  50%|█████     | 10/20 [02:30<01:45, 10.55s/it]
Task 5, Epoch 11/20 => Loss 0.285, Train_accy 76.10, Test_accy 64.92:  55%|█████▌    | 11/20 [02:30<02:12, 14.72s/it]
Task 5, Epoch 12/20 => Loss 0.256, Train_accy 77.25:  55%|█████▌    | 11/20 [02:40<02:12, 14.72s/it]                 
Task 5, Epoch 12/20 => Loss 0.256, Train_accy 77.25:  60%|██████    | 12/20 [02:40<01:46, 13.26s/it]
Task 5, Epoch 13/20 => Loss 0.255, Train_accy 80.11:  60%|██████    | 12/20 [02:50<01:46, 13.26s/it]
Task 5, Epoch 13/20 => Loss 0.255, Train_accy 80.11:  65%|██████▌   | 13/20 [02:50<01:25, 12.23s/it]
Task 5, Epoch 14/20 => Loss 0.160, Train_accy 80.69:  65%|██████▌   | 13/20 [02:57<01:25, 12.23s/it]
Task 5, Epoch 14/20 => Loss 0.160, Train_accy 80.69:  70%|███████   | 14/20 [02:57<01:04, 10.79s/it]
Task 5, Epoch 15/20 => Loss 0.190, Train_accy 78.20:  70%|███████   | 14/20 [03:08<01:04, 10.79s/it]
Task 5, Epoch 15/20 => Loss 0.190, Train_accy 78.20:  75%|███████▌  | 15/20 [03:08<00:53, 10.65s/it]
Task 5, Epoch 16/20 => Loss 0.254, Train_accy 78.78, Test_accy 66.26:  75%|███████▌  | 15/20 [03:29<00:53, 10.65s/it]
Task 5, Epoch 16/20 => Loss 0.254, Train_accy 78.78, Test_accy 66.26:  80%|████████  | 16/20 [03:29<00:55, 13.97s/it]
Task 5, Epoch 17/20 => Loss 0.149, Train_accy 79.54:  80%|████████  | 16/20 [03:40<00:55, 13.97s/it]                 
Task 5, Epoch 17/20 => Loss 0.149, Train_accy 79.54:  85%|████████▌ | 17/20 [03:40<00:38, 12.83s/it]
Task 5, Epoch 18/20 => Loss 0.183, Train_accy 78.97:  85%|████████▌ | 17/20 [03:48<00:38, 12.83s/it]
Task 5, Epoch 18/20 => Loss 0.183, Train_accy 78.97:  90%|█████████ | 18/20 [03:48<00:22, 11.39s/it]
Task 5, Epoch 19/20 => Loss 0.189, Train_accy 80.50:  90%|█████████ | 18/20 [03:59<00:22, 11.39s/it]
Task 5, Epoch 19/20 => Loss 0.189, Train_accy 80.50:  95%|█████████▌| 19/20 [03:59<00:11, 11.46s/it]
Task 5, Epoch 20/20 => Loss 0.191, Train_accy 79.35:  95%|█████████▌| 19/20 [04:09<00:11, 11.46s/it]
Task 5, Epoch 20/20 => Loss 0.191, Train_accy 79.35: 100%|██████████| 20/20 [04:09<00:00, 11.03s/it]
Task 5, Epoch 20/20 => Loss 0.191, Train_accy 79.35: 100%|██████████| 20/20 [04:09<00:00, 12.49s/it]
2025-04-12 15:57:36,468 [finetune.py] => Task 5, Epoch 20/20 => Loss 0.191, Train_accy 79.35
2025-04-12 15:57:47,342 [trainer.py] => No NME accuracy.
2025-04-12 15:57:47,342 [trainer.py] => CNN: {'total': np.float64(66.56), '00-19': np.float64(61.71), '20-39': np.float64(67.03), '40-59': np.float64(67.24), '60-79': np.float64(64.41), '80-99': np.float64(67.83), '100-119': np.float64(73.88), 'old': np.float64(65.4), 'new': np.float64(73.88)}
2025-04-12 15:57:47,342 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92), np.float64(68.97), np.float64(66.56)]
2025-04-12 15:57:47,342 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26), np.float64(88.7), np.float64(87.38)]

Average Accuracy (CNN): 73.775
2025-04-12 15:57:47,342 [trainer.py] => Average Accuracy (CNN): 73.775 

task 6
2025-04-12 15:57:47,344 [trainer.py] => All params: 172058253
2025-04-12 15:57:47,345 [trainer.py] => Trainable params: 460941
2025-04-12 15:57:47,346 [finetune.py] => Learning on 120-140
2025-04-12 15:57:48,574 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 15:57:48,679 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 6, Epoch 1/20 => Loss 3.248, Train_accy 0.00, Test_accy 57.36:   0%|          | 0/20 [00:23<?, ?it/s]
Task 6, Epoch 1/20 => Loss 3.248, Train_accy 0.00, Test_accy 57.36:   5%|▌         | 1/20 [00:23<07:28, 23.61s/it]
Task 6, Epoch 2/20 => Loss 1.587, Train_accy 3.34:   5%|▌         | 1/20 [00:33<07:28, 23.61s/it]                 
Task 6, Epoch 2/20 => Loss 1.587, Train_accy 3.34:  10%|█         | 2/20 [00:33<04:37, 15.43s/it]
Task 6, Epoch 3/20 => Loss 0.803, Train_accy 26.35:  10%|█         | 2/20 [00:44<04:37, 15.43s/it]
Task 6, Epoch 3/20 => Loss 0.803, Train_accy 26.35:  15%|█▌        | 3/20 [00:44<03:48, 13.42s/it]
Task 6, Epoch 4/20 => Loss 0.660, Train_accy 40.63:  15%|█▌        | 3/20 [00:56<03:48, 13.42s/it]
Task 6, Epoch 4/20 => Loss 0.660, Train_accy 40.63:  20%|██        | 4/20 [00:56<03:27, 12.95s/it]
Task 6, Epoch 5/20 => Loss 0.540, Train_accy 50.65:  20%|██        | 4/20 [01:05<03:27, 12.95s/it]
Task 6, Epoch 5/20 => Loss 0.540, Train_accy 50.65:  25%|██▌       | 5/20 [01:05<02:50, 11.36s/it]
Task 6, Epoch 6/20 => Loss 0.432, Train_accy 58.63, Test_accy 63.60:  25%|██▌       | 5/20 [01:28<02:50, 11.36s/it]
Task 6, Epoch 6/20 => Loss 0.432, Train_accy 58.63, Test_accy 63.60:  30%|███       | 6/20 [01:28<03:35, 15.36s/it]
Task 6, Epoch 7/20 => Loss 0.452, Train_accy 66.42:  30%|███       | 6/20 [01:39<03:35, 15.36s/it]                 
Task 6, Epoch 7/20 => Loss 0.452, Train_accy 66.42:  35%|███▌      | 7/20 [01:39<03:01, 13.99s/it]
Task 6, Epoch 8/20 => Loss 0.379, Train_accy 67.72:  35%|███▌      | 7/20 [01:50<03:01, 13.99s/it]
Task 6, Epoch 8/20 => Loss 0.379, Train_accy 67.72:  40%|████      | 8/20 [01:50<02:36, 13.06s/it]
Task 6, Epoch 9/20 => Loss 0.297, Train_accy 67.90:  40%|████      | 8/20 [02:00<02:36, 13.06s/it]
Task 6, Epoch 9/20 => Loss 0.297, Train_accy 67.90:  45%|████▌     | 9/20 [02:00<02:13, 12.14s/it]
Task 6, Epoch 10/20 => Loss 0.386, Train_accy 69.20:  45%|████▌     | 9/20 [02:10<02:13, 12.14s/it]
Task 6, Epoch 10/20 => Loss 0.386, Train_accy 69.20:  50%|█████     | 10/20 [02:10<01:54, 11.48s/it]
Task 6, Epoch 11/20 => Loss 0.256, Train_accy 70.32, Test_accy 62.89:  50%|█████     | 10/20 [02:32<01:54, 11.48s/it]
Task 6, Epoch 11/20 => Loss 0.256, Train_accy 70.32, Test_accy 62.89:  55%|█████▌    | 11/20 [02:32<02:13, 14.81s/it]
Task 6, Epoch 12/20 => Loss 0.330, Train_accy 70.50:  55%|█████▌    | 11/20 [02:42<02:13, 14.81s/it]                 
Task 6, Epoch 12/20 => Loss 0.330, Train_accy 70.50:  60%|██████    | 12/20 [02:42<01:45, 13.20s/it]
Task 6, Epoch 13/20 => Loss 0.258, Train_accy 68.09:  60%|██████    | 12/20 [02:53<01:45, 13.20s/it]
Task 6, Epoch 13/20 => Loss 0.258, Train_accy 68.09:  65%|██████▌   | 13/20 [02:53<01:27, 12.48s/it]
Task 6, Epoch 14/20 => Loss 0.252, Train_accy 70.69:  65%|██████▌   | 13/20 [03:03<01:27, 12.48s/it]
Task 6, Epoch 14/20 => Loss 0.252, Train_accy 70.69:  70%|███████   | 14/20 [03:03<01:09, 11.66s/it]
Task 6, Epoch 15/20 => Loss 0.288, Train_accy 71.80:  70%|███████   | 14/20 [03:12<01:09, 11.66s/it]
Task 6, Epoch 15/20 => Loss 0.288, Train_accy 71.80:  75%|███████▌  | 15/20 [03:12<00:54, 10.84s/it]
Task 6, Epoch 16/20 => Loss 0.223, Train_accy 71.24, Test_accy 64.59:  75%|███████▌  | 15/20 [03:34<00:54, 10.84s/it]
Task 6, Epoch 16/20 => Loss 0.223, Train_accy 71.24, Test_accy 64.59:  80%|████████  | 16/20 [03:34<00:57, 14.27s/it]
Task 6, Epoch 17/20 => Loss 0.174, Train_accy 71.99:  80%|████████  | 16/20 [03:42<00:57, 14.27s/it]                 
Task 6, Epoch 17/20 => Loss 0.174, Train_accy 71.99:  85%|████████▌ | 17/20 [03:42<00:37, 12.49s/it]
Task 6, Epoch 18/20 => Loss 0.167, Train_accy 71.24:  85%|████████▌ | 17/20 [03:55<00:37, 12.49s/it]
Task 6, Epoch 18/20 => Loss 0.167, Train_accy 71.24:  90%|█████████ | 18/20 [03:55<00:25, 12.60s/it]
Task 6, Epoch 19/20 => Loss 0.205, Train_accy 73.10:  90%|█████████ | 18/20 [04:06<00:25, 12.60s/it]
Task 6, Epoch 19/20 => Loss 0.205, Train_accy 73.10:  95%|█████████▌| 19/20 [04:06<00:12, 12.18s/it]
Task 6, Epoch 20/20 => Loss 0.201, Train_accy 76.07:  95%|█████████▌| 19/20 [04:17<00:12, 12.18s/it]
Task 6, Epoch 20/20 => Loss 0.201, Train_accy 76.07: 100%|██████████| 20/20 [04:17<00:00, 11.80s/it]
Task 6, Epoch 20/20 => Loss 0.201, Train_accy 76.07: 100%|██████████| 20/20 [04:17<00:00, 12.88s/it]
2025-04-12 16:02:06,977 [finetune.py] => Task 6, Epoch 20/20 => Loss 0.201, Train_accy 76.07
2025-04-12 16:02:17,383 [trainer.py] => No NME accuracy.
2025-04-12 16:02:17,383 [trainer.py] => CNN: {'total': np.float64(64.41), '00-19': np.float64(60.0), '20-39': np.float64(63.24), '40-59': np.float64(66.38), '60-79': np.float64(60.36), '80-99': np.float64(66.43), '100-119': np.float64(74.63), '120-139': np.float64(64.38), 'old': np.float64(64.41), 'new': np.float64(64.38)}
2025-04-12 16:02:17,383 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92), np.float64(68.97), np.float64(66.56), np.float64(64.41)]
2025-04-12 16:02:17,383 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26), np.float64(88.7), np.float64(87.38), np.float64(85.73)]

Average Accuracy (CNN): 72.43714285714286
2025-04-12 16:02:17,384 [trainer.py] => Average Accuracy (CNN): 72.43714285714286 

task 7
2025-04-12 16:02:17,398 [trainer.py] => All params: 172073633
2025-04-12 16:02:17,399 [trainer.py] => Trainable params: 476321
2025-04-12 16:02:17,400 [finetune.py] => Learning on 140-160
2025-04-12 16:02:22,970 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 16:02:23,048 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 7, Epoch 1/20 => Loss 3.396, Train_accy 0.00, Test_accy 58.09:   0%|          | 0/20 [00:21<?, ?it/s]
Task 7, Epoch 1/20 => Loss 3.396, Train_accy 0.00, Test_accy 58.09:   5%|▌         | 1/20 [00:21<06:50, 21.61s/it]
Task 7, Epoch 2/20 => Loss 1.715, Train_accy 6.40:   5%|▌         | 1/20 [00:32<06:50, 21.61s/it]                 
Task 7, Epoch 2/20 => Loss 1.715, Train_accy 6.40:  10%|█         | 2/20 [00:32<04:38, 15.49s/it]
Task 7, Epoch 3/20 => Loss 0.954, Train_accy 26.55:  10%|█         | 2/20 [00:43<04:38, 15.49s/it]
Task 7, Epoch 3/20 => Loss 0.954, Train_accy 26.55:  15%|█▌        | 3/20 [00:43<03:48, 13.45s/it]
Task 7, Epoch 4/20 => Loss 0.626, Train_accy 41.43:  15%|█▌        | 3/20 [00:55<03:48, 13.45s/it]
Task 7, Epoch 4/20 => Loss 0.626, Train_accy 41.43:  20%|██        | 4/20 [00:55<03:26, 12.92s/it]
Task 7, Epoch 5/20 => Loss 0.479, Train_accy 51.79:  20%|██        | 4/20 [01:05<03:26, 12.92s/it]
Task 7, Epoch 5/20 => Loss 0.479, Train_accy 51.79:  25%|██▌       | 5/20 [01:05<02:54, 11.64s/it]
Task 7, Epoch 6/20 => Loss 0.320, Train_accy 59.70, Test_accy 61.22:  25%|██▌       | 5/20 [01:28<02:54, 11.64s/it]
Task 7, Epoch 6/20 => Loss 0.320, Train_accy 59.70, Test_accy 61.22:  30%|███       | 6/20 [01:28<03:36, 15.48s/it]
Task 7, Epoch 7/20 => Loss 0.420, Train_accy 64.22:  30%|███       | 6/20 [01:39<03:36, 15.48s/it]                 
Task 7, Epoch 7/20 => Loss 0.420, Train_accy 64.22:  35%|███▌      | 7/20 [01:39<03:02, 14.02s/it]
Task 7, Epoch 8/20 => Loss 0.308, Train_accy 65.35:  35%|███▌      | 7/20 [01:48<03:02, 14.02s/it]
Task 7, Epoch 8/20 => Loss 0.308, Train_accy 65.35:  40%|████      | 8/20 [01:48<02:28, 12.35s/it]
Task 7, Epoch 9/20 => Loss 0.400, Train_accy 66.67:  40%|████      | 8/20 [01:58<02:28, 12.35s/it]
Task 7, Epoch 9/20 => Loss 0.400, Train_accy 66.67:  45%|████▌     | 9/20 [01:58<02:10, 11.86s/it]
Task 7, Epoch 10/20 => Loss 0.349, Train_accy 67.04:  45%|████▌     | 9/20 [02:10<02:10, 11.86s/it]
Task 7, Epoch 10/20 => Loss 0.349, Train_accy 67.04:  50%|█████     | 10/20 [02:10<01:57, 11.72s/it]
Task 7, Epoch 11/20 => Loss 0.340, Train_accy 67.80, Test_accy 61.70:  50%|█████     | 10/20 [02:32<01:57, 11.72s/it]
Task 7, Epoch 11/20 => Loss 0.340, Train_accy 67.80, Test_accy 61.70:  55%|█████▌    | 11/20 [02:32<02:14, 14.94s/it]
Task 7, Epoch 12/20 => Loss 0.306, Train_accy 72.69:  55%|█████▌    | 11/20 [02:42<02:14, 14.94s/it]                 
Task 7, Epoch 12/20 => Loss 0.306, Train_accy 72.69:  60%|██████    | 12/20 [02:42<01:48, 13.53s/it]
Task 7, Epoch 13/20 => Loss 0.328, Train_accy 69.30:  60%|██████    | 12/20 [02:53<01:48, 13.53s/it]
Task 7, Epoch 13/20 => Loss 0.328, Train_accy 69.30:  65%|██████▌   | 13/20 [02:53<01:28, 12.61s/it]
Task 7, Epoch 14/20 => Loss 0.293, Train_accy 70.24:  65%|██████▌   | 13/20 [03:04<01:28, 12.61s/it]
Task 7, Epoch 14/20 => Loss 0.293, Train_accy 70.24:  70%|███████   | 14/20 [03:04<01:12, 12.12s/it]
Task 7, Epoch 15/20 => Loss 0.202, Train_accy 73.82:  70%|███████   | 14/20 [03:12<01:12, 12.12s/it]
Task 7, Epoch 15/20 => Loss 0.202, Train_accy 73.82:  75%|███████▌  | 15/20 [03:12<00:55, 11.10s/it]
Task 7, Epoch 16/20 => Loss 0.244, Train_accy 68.55, Test_accy 61.70:  75%|███████▌  | 15/20 [03:33<00:55, 11.10s/it]
Task 7, Epoch 16/20 => Loss 0.244, Train_accy 68.55, Test_accy 61.70:  80%|████████  | 16/20 [03:33<00:55, 13.94s/it]
Task 7, Epoch 17/20 => Loss 0.230, Train_accy 71.19:  80%|████████  | 16/20 [03:40<00:55, 13.94s/it]                 
Task 7, Epoch 17/20 => Loss 0.230, Train_accy 71.19:  85%|████████▌ | 17/20 [03:40<00:35, 11.86s/it]
Task 7, Epoch 18/20 => Loss 0.164, Train_accy 73.26:  85%|████████▌ | 17/20 [03:47<00:35, 11.86s/it]
Task 7, Epoch 18/20 => Loss 0.164, Train_accy 73.26:  90%|█████████ | 18/20 [03:47<00:21, 10.51s/it]
Task 7, Epoch 19/20 => Loss 0.214, Train_accy 70.81:  90%|█████████ | 18/20 [03:55<00:21, 10.51s/it]
Task 7, Epoch 19/20 => Loss 0.214, Train_accy 70.81:  95%|█████████▌| 19/20 [03:55<00:09,  9.64s/it]
Task 7, Epoch 20/20 => Loss 0.208, Train_accy 74.58:  95%|█████████▌| 19/20 [04:03<00:09,  9.64s/it]
Task 7, Epoch 20/20 => Loss 0.208, Train_accy 74.58: 100%|██████████| 20/20 [04:03<00:00,  8.99s/it]
Task 7, Epoch 20/20 => Loss 0.208, Train_accy 74.58: 100%|██████████| 20/20 [04:03<00:00, 12.15s/it]
2025-04-12 16:06:26,811 [finetune.py] => Task 7, Epoch 20/20 => Loss 0.208, Train_accy 74.58
2025-04-12 16:06:36,890 [trainer.py] => No NME accuracy.
2025-04-12 16:06:36,891 [trainer.py] => CNN: {'total': np.float64(61.7), '00-19': np.float64(56.0), '20-39': np.float64(60.0), '40-59': np.float64(64.66), '60-79': np.float64(58.56), '80-99': np.float64(63.64), '100-119': np.float64(73.13), '120-139': np.float64(63.01), '140-159': np.float64(59.06), 'old': np.float64(62.0), 'new': np.float64(59.06)}
2025-04-12 16:06:36,891 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92), np.float64(68.97), np.float64(66.56), np.float64(64.41), np.float64(61.7)]
2025-04-12 16:06:36,891 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26), np.float64(88.7), np.float64(87.38), np.float64(85.73), np.float64(84.46)]

Average Accuracy (CNN): 71.09500000000001
2025-04-12 16:06:36,891 [trainer.py] => Average Accuracy (CNN): 71.09500000000001 

task 8
2025-04-12 16:06:36,893 [trainer.py] => All params: 172089013
2025-04-12 16:06:36,894 [trainer.py] => Trainable params: 491701
2025-04-12 16:06:36,895 [finetune.py] => Learning on 160-180
2025-04-12 16:06:37,739 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 16:06:37,811 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 8, Epoch 1/20 => Loss 3.410, Train_accy 0.18, Test_accy 54.12:   0%|          | 0/20 [00:18<?, ?it/s]
Task 8, Epoch 1/20 => Loss 3.410, Train_accy 0.18, Test_accy 54.12:   5%|▌         | 1/20 [00:18<05:56, 18.74s/it]
Task 8, Epoch 2/20 => Loss 1.833, Train_accy 2.28:   5%|▌         | 1/20 [00:26<05:56, 18.74s/it]                 
Task 8, Epoch 2/20 => Loss 1.833, Train_accy 2.28:  10%|█         | 2/20 [00:26<03:45, 12.51s/it]
Task 8, Epoch 3/20 => Loss 1.159, Train_accy 15.06:  10%|█         | 2/20 [00:34<03:45, 12.51s/it]
Task 8, Epoch 3/20 => Loss 1.159, Train_accy 15.06:  15%|█▌        | 3/20 [00:34<02:56, 10.41s/it]
Task 8, Epoch 4/20 => Loss 0.749, Train_accy 33.80:  15%|█▌        | 3/20 [00:42<02:56, 10.41s/it]
Task 8, Epoch 4/20 => Loss 0.749, Train_accy 33.80:  20%|██        | 4/20 [00:42<02:30,  9.38s/it]
Task 8, Epoch 5/20 => Loss 0.624, Train_accy 39.40:  20%|██        | 4/20 [00:50<02:30,  9.38s/it]
Task 8, Epoch 5/20 => Loss 0.624, Train_accy 39.40:  25%|██▌       | 5/20 [00:50<02:11,  8.77s/it]
Task 8, Epoch 6/20 => Loss 0.562, Train_accy 48.86, Test_accy 59.07:  25%|██▌       | 5/20 [01:09<02:11,  8.77s/it]
Task 8, Epoch 6/20 => Loss 0.562, Train_accy 48.86, Test_accy 59.07:  30%|███       | 6/20 [01:09<02:50, 12.17s/it]
Task 8, Epoch 7/20 => Loss 0.486, Train_accy 51.14:  30%|███       | 6/20 [01:17<02:50, 12.17s/it]                 
Task 8, Epoch 7/20 => Loss 0.486, Train_accy 51.14:  35%|███▌      | 7/20 [01:17<02:21, 10.86s/it]
Task 8, Epoch 8/20 => Loss 0.440, Train_accy 50.96:  35%|███▌      | 7/20 [01:25<02:21, 10.86s/it]
Task 8, Epoch 8/20 => Loss 0.440, Train_accy 50.96:  40%|████      | 8/20 [01:25<01:59,  9.97s/it]
Task 8, Epoch 9/20 => Loss 0.411, Train_accy 54.29:  40%|████      | 8/20 [01:33<01:59,  9.97s/it]
Task 8, Epoch 9/20 => Loss 0.411, Train_accy 54.29:  45%|████▌     | 9/20 [01:33<01:43,  9.38s/it]
Task 8, Epoch 10/20 => Loss 0.419, Train_accy 52.89:  45%|████▌     | 9/20 [01:41<01:43,  9.38s/it]
Task 8, Epoch 10/20 => Loss 0.419, Train_accy 52.89:  50%|█████     | 10/20 [01:41<01:30,  9.05s/it]
Task 8, Epoch 11/20 => Loss 0.289, Train_accy 60.60, Test_accy 59.71:  50%|█████     | 10/20 [01:59<01:30,  9.05s/it]
Task 8, Epoch 11/20 => Loss 0.289, Train_accy 60.60, Test_accy 59.71:  55%|█████▌    | 11/20 [01:59<01:46, 11.78s/it]
Task 8, Epoch 12/20 => Loss 0.291, Train_accy 57.97:  55%|█████▌    | 11/20 [02:07<01:46, 11.78s/it]                 
Task 8, Epoch 12/20 => Loss 0.291, Train_accy 57.97:  60%|██████    | 12/20 [02:07<01:24, 10.52s/it]
Task 8, Epoch 13/20 => Loss 0.350, Train_accy 58.67:  60%|██████    | 12/20 [02:15<01:24, 10.52s/it]
Task 8, Epoch 13/20 => Loss 0.350, Train_accy 58.67:  65%|██████▌   | 13/20 [02:15<01:08,  9.78s/it]
Task 8, Epoch 14/20 => Loss 0.247, Train_accy 60.60:  65%|██████▌   | 13/20 [02:23<01:08,  9.78s/it]
Task 8, Epoch 14/20 => Loss 0.247, Train_accy 60.60:  70%|███████   | 14/20 [02:23<00:55,  9.21s/it]
Task 8, Epoch 15/20 => Loss 0.325, Train_accy 58.67:  70%|███████   | 14/20 [02:31<00:55,  9.21s/it]
Task 8, Epoch 15/20 => Loss 0.325, Train_accy 58.67:  75%|███████▌  | 15/20 [02:31<00:43,  8.78s/it]
Task 8, Epoch 16/20 => Loss 0.294, Train_accy 60.07, Test_accy 60.50:  75%|███████▌  | 15/20 [02:48<00:43,  8.78s/it]
Task 8, Epoch 16/20 => Loss 0.294, Train_accy 60.07, Test_accy 60.50:  80%|████████  | 16/20 [02:48<00:45, 11.47s/it]
Task 8, Epoch 17/20 => Loss 0.277, Train_accy 60.77:  80%|████████  | 16/20 [02:57<00:45, 11.47s/it]                 
Task 8, Epoch 17/20 => Loss 0.277, Train_accy 60.77:  85%|████████▌ | 17/20 [02:57<00:31, 10.60s/it]
Task 8, Epoch 18/20 => Loss 0.261, Train_accy 60.60:  85%|████████▌ | 17/20 [03:05<00:31, 10.60s/it]
Task 8, Epoch 18/20 => Loss 0.261, Train_accy 60.60:  90%|█████████ | 18/20 [03:05<00:19,  9.95s/it]
Task 8, Epoch 19/20 => Loss 0.245, Train_accy 62.35:  90%|█████████ | 18/20 [03:13<00:19,  9.95s/it]
Task 8, Epoch 19/20 => Loss 0.245, Train_accy 62.35:  95%|█████████▌| 19/20 [03:13<00:09,  9.23s/it]
Task 8, Epoch 20/20 => Loss 0.245, Train_accy 63.22:  95%|█████████▌| 19/20 [03:21<00:09,  9.23s/it]
Task 8, Epoch 20/20 => Loss 0.245, Train_accy 63.22: 100%|██████████| 20/20 [03:21<00:00,  8.77s/it]
Task 8, Epoch 20/20 => Loss 0.245, Train_accy 63.22: 100%|██████████| 20/20 [03:21<00:00, 10.05s/it]
2025-04-12 16:09:59,086 [finetune.py] => Task 8, Epoch 20/20 => Loss 0.245, Train_accy 63.22
2025-04-12 16:10:09,253 [trainer.py] => No NME accuracy.
2025-04-12 16:10:09,253 [trainer.py] => CNN: {'total': np.float64(60.29), '00-19': np.float64(56.57), '20-39': np.float64(56.76), '40-59': np.float64(62.93), '60-79': np.float64(58.11), '80-99': np.float64(63.64), '100-119': np.float64(71.64), '120-139': np.float64(62.33), '140-159': np.float64(59.84), '160-179': np.float64(55.1), 'old': np.float64(60.9), 'new': np.float64(55.1)}
2025-04-12 16:10:09,253 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92), np.float64(68.97), np.float64(66.56), np.float64(64.41), np.float64(61.7), np.float64(60.29)]
2025-04-12 16:10:09,253 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26), np.float64(88.7), np.float64(87.38), np.float64(85.73), np.float64(84.46), np.float64(83.37)]

Average Accuracy (CNN): 69.89444444444445
2025-04-12 16:10:09,253 [trainer.py] => Average Accuracy (CNN): 69.89444444444445 

task 9
2025-04-12 16:10:09,255 [trainer.py] => All params: 172104393
2025-04-12 16:10:09,256 [trainer.py] => Trainable params: 507081
2025-04-12 16:10:09,257 [finetune.py] => Learning on 180-200
2025-04-12 16:10:10,104 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-04-12 16:10:10,169 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./ImageNetA1/

  0%|          | 0/20 [00:00<?, ?it/s]
Task 9, Epoch 1/20 => Loss 3.053, Train_accy 0.19, Test_accy 55.10:   0%|          | 0/20 [00:19<?, ?it/s]
Task 9, Epoch 1/20 => Loss 3.053, Train_accy 0.19, Test_accy 55.10:   5%|▌         | 1/20 [00:19<06:01, 19.02s/it]
Task 9, Epoch 2/20 => Loss 1.463, Train_accy 5.31:   5%|▌         | 1/20 [00:26<06:01, 19.02s/it]                 
Task 9, Epoch 2/20 => Loss 1.463, Train_accy 5.31:  10%|█         | 2/20 [00:26<03:41, 12.31s/it]
Task 9, Epoch 3/20 => Loss 0.895, Train_accy 31.31:  10%|█         | 2/20 [00:34<03:41, 12.31s/it]
Task 9, Epoch 3/20 => Loss 0.895, Train_accy 31.31:  15%|█▌        | 3/20 [00:34<02:52, 10.15s/it]
Task 9, Epoch 4/20 => Loss 0.646, Train_accy 43.45:  15%|█▌        | 3/20 [00:41<02:52, 10.15s/it]
Task 9, Epoch 4/20 => Loss 0.646, Train_accy 43.45:  20%|██        | 4/20 [00:41<02:25,  9.07s/it]
Task 9, Epoch 5/20 => Loss 0.404, Train_accy 54.65:  20%|██        | 4/20 [00:49<02:25,  9.07s/it]
Task 9, Epoch 5/20 => Loss 0.404, Train_accy 54.65:  25%|██▌       | 5/20 [00:49<02:08,  8.60s/it]
Task 9, Epoch 6/20 => Loss 0.459, Train_accy 61.48, Test_accy 58.46:  25%|██▌       | 5/20 [01:08<02:08,  8.60s/it]
Task 9, Epoch 6/20 => Loss 0.459, Train_accy 61.48, Test_accy 58.46:  30%|███       | 6/20 [01:08<02:51, 12.25s/it]
Task 9, Epoch 7/20 => Loss 0.389, Train_accy 64.33:  30%|███       | 6/20 [01:16<02:51, 12.25s/it]                 
Task 9, Epoch 7/20 => Loss 0.389, Train_accy 64.33:  35%|███▌      | 7/20 [01:16<02:20, 10.78s/it]
Task 9, Epoch 8/20 => Loss 0.370, Train_accy 67.55:  35%|███▌      | 7/20 [01:23<02:20, 10.78s/it]
Task 9, Epoch 8/20 => Loss 0.370, Train_accy 67.55:  40%|████      | 8/20 [01:23<01:55,  9.63s/it]
Task 9, Epoch 9/20 => Loss 0.368, Train_accy 68.50:  40%|████      | 8/20 [01:31<01:55,  9.63s/it]
Task 9, Epoch 9/20 => Loss 0.368, Train_accy 68.50:  45%|████▌     | 9/20 [01:31<01:39,  9.05s/it]
Task 9, Epoch 10/20 => Loss 0.242, Train_accy 71.54:  45%|████▌     | 9/20 [01:39<01:39,  9.05s/it]
Task 9, Epoch 10/20 => Loss 0.242, Train_accy 71.54:  50%|█████     | 10/20 [01:39<01:27,  8.78s/it]
Task 9, Epoch 11/20 => Loss 0.282, Train_accy 71.73, Test_accy 58.66:  50%|█████     | 10/20 [01:58<01:27,  8.78s/it]
Task 9, Epoch 11/20 => Loss 0.282, Train_accy 71.73, Test_accy 58.66:  55%|█████▌    | 11/20 [01:58<01:47, 11.90s/it]
Task 9, Epoch 12/20 => Loss 0.252, Train_accy 70.21:  55%|█████▌    | 11/20 [02:06<01:47, 11.90s/it]                 
Task 9, Epoch 12/20 => Loss 0.252, Train_accy 70.21:  60%|██████    | 12/20 [02:06<01:25, 10.67s/it]
Task 9, Epoch 13/20 => Loss 0.238, Train_accy 73.62:  60%|██████    | 12/20 [02:14<01:25, 10.67s/it]
Task 9, Epoch 13/20 => Loss 0.238, Train_accy 73.62:  65%|██████▌   | 13/20 [02:14<01:08,  9.82s/it]
Task 9, Epoch 14/20 => Loss 0.176, Train_accy 76.28:  65%|██████▌   | 13/20 [02:21<01:08,  9.82s/it]
Task 9, Epoch 14/20 => Loss 0.176, Train_accy 76.28:  70%|███████   | 14/20 [02:21<00:55,  9.17s/it]
Task 9, Epoch 15/20 => Loss 0.242, Train_accy 73.62:  70%|███████   | 14/20 [02:30<00:55,  9.17s/it]
Task 9, Epoch 15/20 => Loss 0.242, Train_accy 73.62:  75%|███████▌  | 15/20 [02:30<00:44,  8.86s/it]
Task 9, Epoch 16/20 => Loss 0.285, Train_accy 76.85, Test_accy 58.59:  75%|███████▌  | 15/20 [02:49<00:44,  8.86s/it]
Task 9, Epoch 16/20 => Loss 0.285, Train_accy 76.85, Test_accy 58.59:  80%|████████  | 16/20 [02:49<00:47, 11.96s/it]
Task 9, Epoch 17/20 => Loss 0.196, Train_accy 77.23:  80%|████████  | 16/20 [02:57<00:47, 11.96s/it]                 
Task 9, Epoch 17/20 => Loss 0.196, Train_accy 77.23:  85%|████████▌ | 17/20 [02:57<00:32, 10.74s/it]
Task 9, Epoch 18/20 => Loss 0.139, Train_accy 74.38:  85%|████████▌ | 17/20 [03:04<00:32, 10.74s/it]
Task 9, Epoch 18/20 => Loss 0.139, Train_accy 74.38:  90%|█████████ | 18/20 [03:04<00:19,  9.82s/it]
Task 9, Epoch 19/20 => Loss 0.212, Train_accy 78.18:  90%|█████████ | 18/20 [03:12<00:19,  9.82s/it]
Task 9, Epoch 19/20 => Loss 0.212, Train_accy 78.18:  95%|█████████▌| 19/20 [03:12<00:09,  9.16s/it]
Task 9, Epoch 20/20 => Loss 0.202, Train_accy 75.52:  95%|█████████▌| 19/20 [03:20<00:09,  9.16s/it]
Task 9, Epoch 20/20 => Loss 0.202, Train_accy 75.52: 100%|██████████| 20/20 [03:20<00:00,  8.81s/it]
Task 9, Epoch 20/20 => Loss 0.202, Train_accy 75.52: 100%|██████████| 20/20 [03:20<00:00, 10.02s/it]
2025-04-12 16:13:30,969 [finetune.py] => Task 9, Epoch 20/20 => Loss 0.202, Train_accy 75.52
2025-04-12 16:13:41,892 [trainer.py] => No NME accuracy.
2025-04-12 16:13:41,892 [trainer.py] => CNN: {'total': np.float64(58.66), '00-19': np.float64(53.71), '20-39': np.float64(55.68), '40-59': np.float64(64.66), '60-79': np.float64(54.5), '80-99': np.float64(62.94), '100-119': np.float64(67.16), '120-139': np.float64(62.33), '140-159': np.float64(59.84), '160-179': np.float64(55.1), '180-199': np.float64(56.45), 'old': np.float64(58.85), 'new': np.float64(56.45)}
2025-04-12 16:13:41,892 [trainer.py] => CNN top1 curve: [np.float64(82.29), np.float64(78.33), np.float64(74.58), np.float64(71.92), np.float64(68.97), np.float64(66.56), np.float64(64.41), np.float64(61.7), np.float64(60.29), np.float64(58.66)]
2025-04-12 16:13:41,892 [trainer.py] => CNN top5 curve: [np.float64(97.71), np.float64(94.72), np.float64(91.6), np.float64(91.26), np.float64(88.7), np.float64(87.38), np.float64(85.73), np.float64(84.46), np.float64(83.37), np.float64(82.16)]

Average Accuracy (CNN): 68.771
2025-04-12 16:13:41,893 [trainer.py] => Average Accuracy (CNN): 68.771 

Accuracy Matrix (CNN):
[[82.29 78.86 75.43 70.86 65.71 61.71 60.   56.   56.57 53.71]
 [ 0.   77.84 72.97 71.89 70.27 73.88 74.63 73.13 71.64 67.16]
 [ 0.    0.   75.86 75.   69.83 67.03 64.38 63.01 62.33 62.33]
 [ 0.    0.    0.   71.17 70.27 67.24 63.24 59.06 59.84 59.84]
 [ 0.    0.    0.    0.   68.53 64.41 66.38 60.   55.1  55.1 ]
 [ 0.    0.    0.    0.    0.   67.83 60.36 64.66 56.76 56.45]
 [ 0.    0.    0.    0.    0.    0.   66.43 58.56 62.93 55.68]
 [ 0.    0.    0.    0.    0.    0.    0.   63.64 58.11 64.66]
 [ 0.    0.    0.    0.    0.    0.    0.    0.   63.64 54.5 ]
 [ 0.    0.    0.    0.    0.    0.    0.    0.    0.   62.94]]
2025-04-12 16:13:41,893 [trainer.py] => Forgetting (CNN): 12.091111111111111
